### RNN
We use the [RNN library](https://github.com/Element-Research/rnn) provided by Element-Research. This *RNN_LSTM.lua* takes computed feature vectors from each of the frames, and learns a generic model to represent the videos aross all training video frames. 

Using command line arguments or modify the code in *RNN_LSTM.lua*, you can essentially modify the Learning rate, batch size, CUDA options, LSTM or GRU, dropout layers, and the dimension of hidden layers. Note that, currently there are many parameters being declared, but not all the parameters will be used for now. 

### Implementation
All of the codes were implemented by [Chih-Yao Ma](http://shallowdown.wix.com/chih-yao-ma). Some of the codes referred to the example codes provided by [Element-Research](https://github.com/Element-Research/rnn/tree/master/examples). I basically referred to all the example codes, and rewrite a single *RNN_LSTM.lua* file for training and validation testing. After I confirmed the code is working, I divided the codes into four parts: RNN_LSTM, data, train, and test. The formatting of these codes and functions are referred to the examples provided for our homework assignments for CNN. 

---
## Usage for CNN with RNN
We use the [RNN library](https://github.com/Element-Research/rnn) provided by Element-Research. Simply install it by: 
```bash
$ luarocks install rnn
```
The RNN will take the **feature vectors** generated by the first CNN as input for training. For downloading the feature vectors generated by ourselves, please refer to the Dropbox link below. Theses are the exactly training and testing list from UCF101. If you would like to compare with our results, please use the same training and testing list, as it will affect your overall performance a lot. 

* [Features for training](https://www.dropbox.com/s/b0gbo7psouxeu2c/data_UCF101_train_1.t7?dl=0)
* [Features for testing](https://www.dropbox.com/s/98fr9df1r4nl18v/data_UCF101_test_1.t7?dl=0)

After you downloaded the feature vectors, please modify the code in *./RNN/data.lua* to the director where you put your feature vector files. 

To start the training process, simple execute: 
```bash
$ th RNN_LSTM.lua
```

The training and testing performance will be plotted, and the results will be saved into log files. The learning rate and best testing accuracy will be reported each epoch if there is any update. 


---
### Note 
This is rather a note for me to remember what experiments have been done and what needs to be done in order to moving forward. 

---
TODO: 
- [ ] Experiment with different number of frames for training
- [x] Experiment with different batch size and learning rate
- [x] Train with different optimizers, like sgd, adam, adamax, rmsprop
- [x] Comparison between GRU and LSTM, but they should pretty much perform the same
- [ ] Have all the feature vectors from all frames for testing, not just 50 frames per video 
- [x] During testing, if you have at least two frames, input for the network should be feature vector from 1 to t-1
- [x] Feedforward from each time step and average across all frames over a video to make prediction
- [ ] Instead of averaging across all frames per video, can we have weightings?


Note: 

Optimizer: tried SGD and Adam. It gives faster and so far the best convergence. it requires relatively smaller learning rate compared with SGD. I am using **LearningRate = 5e-4** for Adam. 

Hidden layers: using hidden number all the way to 128 (1024, 768, 512, 256, 128) seems to be overfitting the data. With the feature vector dimension to be 1024, the number of hidden layers (1024, 512, 256) can achieve 92% accuracy. 

UCF101 training and testing list: this training and testing list completely ruined the overall performance. Using hidden layers with (1024, 512, 256) only get closed to 60% accuracy. 


#### Contact: [Chih-Yao Ma](http://shallowdown.wix.com/chih-yao-ma) at <cyma@gatech.edu>
