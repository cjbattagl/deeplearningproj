-- Georgia Institute of Technology 
-- CS8803DL Spring 2016 (Instructor: Zsolt Kira)
-- Final Project: Deep Learning for Video Classification

-- Generate the name list corresponding to the features generated by "run_UCF101_final.lua"
-- follow the split sets provided in the UCF-101 website

-- Reference:
-- Khurram Soomro, Amir Roshan Zamir and Mubarak Shah, 
-- "UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild.", 
-- CRCV-TR-12-01, November, 2012. 

-- ffmpeg usage:
-- Video{
--     [path = string]          -- path to video
--     [width = number]         -- width  [default = 224]
--     [height = number]        -- height  [default = 224]
--     [zoom = number]          -- zoom factor  [default = 1]
--     [fps = number]           -- frames per second  [default = 25]
--     [length = number]        -- length, in seconds  [default = 2]
--     [seek = number]          -- seek to pos. in seconds  [default = 0]
--     [channel = number]       -- video channel  [default = 0]
--     [load = boolean]         -- loads frames after conversion  [default = true]
--     [delete = boolean]       -- clears (rm) frames after load  [default = true]
--     [encoding = string]      -- format of dumped frames  [default = png]
--     [tensor = torch.Tensor]  -- provide a packed tensor (NxCxHxW or NxHxW), that bypasses path
--     [destFolder = string]    -- destination folder  [default = out_frames]
--     [silent = boolean]       -- suppress output  [default = false]
-- }

-- author: Min-Hung Chen
-- contact: cmhungsteve@gatech.edu
-- Last updated: 04/08/2016

require 'xlua'
require 'torch'
--require 'imgraph'
--require 'nnx'
require 'ffmpeg'
--require 'image'
--require 'nn'

----------------------------------------------
--         Input/Output information         --
----------------------------------------------
-- select the number of classes, groups & videos you want to use
numClass = 101
dimFeat = 1024

----------------------------------------------
-- 				Data paths				    --
----------------------------------------------
dirModel = './models/'
dirDatabase = '/home/cmhung/Desktop/UCF-101/'

----------------------------------------------
-- 			User-defined parameters			--
----------------------------------------------
numFrameMin = 50
numSplit = 3

-- Train/Test split
groupSplit = {}
table.insert(groupSplit, {setTr = torch.Tensor({{8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25}}),
setTe = torch.Tensor({{1,2,3,4,5,6,7}})})
table.insert(groupSplit, {setTr = torch.Tensor({{1,2,3,4,5,6,7,15,16,17,18,19,20,21,22,23,24,25}}),
setTe = torch.Tensor({{8,9,10,11,12,13,14}})})
table.insert(groupSplit, {setTr = torch.Tensor({{1,2,3,4,5,6,7,8,9,10,11,12,13,14,22,23,24,25}}),
setTe = torch.Tensor({{15,16,17,18,19,20,21}})})

-- Output information --
outTrain = {}
for sp=1,numSplit do
	table.insert(outTrain, {name = 'name_UCF101_train_'..sp..'.t7'})
end

outTest = {}
for sp=1,numSplit do
	table.insert(outTest, {name = 'name_UCF101_test_'..sp..'.t7'})
end

----------------
-- parse args --
----------------
op = xlua.OptionParser('%prog [options]')
op:option{'-f', '--fps', action='store', dest='fps',
          help='number of frames per second', default=25}
op:option{'-t', '--time', action='store', dest='seconds',
          help='length to process (in seconds)', default=2}
op:option{'-w', '--width', action='store', dest='width',
          help='resize video, width', default=224}
op:option{'-h', '--height', action='store', dest='height',
          help='resize video, height', default=224}
op:option{'-z', '--zoom', action='store', dest='zoom',
          help='display zoom', default=1}
op:option{'-p', '--type', action='store', dest='type',
          help='option for CPU/GPU', default='cuda'}
op:option{'-i', '--devid', action='store', dest='devid',
          help='device ID (if using CUDA)', default=1}      
opt,args = op:parse()

----------------------------------------------
-- 					Class		        	--
----------------------------------------------
-- -- Read textClass --
-- classAll = {}
-- for l in io.lines(dirDatabase..textClass) do
-- 	local n, c = l:match '(%S+)%s+(%S+)%s'
--     table.insert(classAll, {num = tonumber(n), name = c})
-- end

nameClass = paths.dir(dirDatabase) 
numClassTotal = #nameClass -- 101 classes + "." + ".."

-- ----------------------------------------------
-- -- 					Models		        	--
-- ----------------------------------------------
-- ------ Loading the model ------
-- print ' '
-- print '==> Loading the model...'
-- -- 1. Torch model
-- net = torch.load(modelName):unpack():float()

-- -- -- 2. Caffe model
-- -- net = loadcaffe.load(prototxt, binary):float()

-- net:evaluate()

-- ------ model modification ------
-- net:remove(30) -- process the model

-- print(net)
-- print ' '

----------------------------------------------
--  		       GPU option	 	        --
----------------------------------------------
if opt.type == 'cuda' then
  print(sys.COLORS.red ..  '==> switching to CUDA')
  require 'cunn'
  --net:cuda()
  cutorch.setDevice(opt.devid)
  print(sys.COLORS.red ..  '==> using GPU #' .. cutorch.getDevice())
  --images = images:cuda()
-- else
--   net:float()
end
print(sys.COLORS.white ..  ' ')

-- ----------------------------------------------
-- -- 			 Loading ImageNet labels		--
-- ----------------------------------------------
-- print '==> Loading the synsets...'
-- print 'Loads mapping from net outputs to human readable labels'
-- synset_words = {}
-- for line in io.lines'synset_words.txt' do table.insert(synset_words, line:sub(11)) end

print ' '
--====================================================================--
--                     Run all the videos in UCF-101                  --
--====================================================================--
print '==> Processing all the videos...'

-- featMats = torch.DoubleTensor(numVideo, dimFeat, numFrameMin):zero()
-- labels = torch.DoubleTensor(numVideo):zero()

-- Load the intermediate feature data or generate a new one --
for sp=1,numSplit do
	-- Training data --
	if not paths.filep(outTrain[sp].name) then
		Tr = {} -- output
		Tr.name = {}
		Tr.path = {}
		Tr.countVideo = 0
		Tr.countClass = 0
		Tr.c_finished = 0 -- different from countClass since there are also "." and ".."
	else
		Tr = torch.load(outTrain[sp].name) -- output
	end

	-- Testing data --
	if not paths.filep(outTest[sp].name) then
		Te = {} -- output
		Te.name = {}
		Te.path = {}
		Te.countVideo = 0
		Te.countClass = 0
		Te.c_finished = 0 -- different from countClass since there are also "." and ".."
	else
		Te = torch.load(outTest[sp].name) -- output
	end
	collectgarbage()

	timerAll = torch.Timer() -- count the whole processing time

	if Tr.countClass == numClass and Te.countClass == numClass then
		print('The feature data of split '..sp..' is already in your folder!!!!!!')
	else
		for c=Tr.c_finished+1, numClassTotal do
			if nameClass[c] ~= '.' and nameClass[c] ~= '..' then
				print('Current Class: '..c..'. '..nameClass[c])
				Tr.countClass = Tr.countClass + 1
				Te.countClass = Te.countClass + 1
			  	------ Data paths ------
			  	local dirClass = dirDatabase..nameClass[c]..'/' 
			  	local nameSubVideo = paths.dir(dirClass)
			  	local numSubVideoTotal = #nameSubVideo -- videos + '.' + '..'
			  	--
			  	local timerClass = torch.Timer() -- count the processing time for one class
			  	--
			    for sv=1, numSubVideoTotal do
			      	--------------------
			      	-- Load the video --
			      	--------------------  
			      	if nameSubVideo[sv] ~= '.' and nameSubVideo[sv] ~= '..' then
			        	local videoName = nameSubVideo[sv]
			        	local videoPath = dirClass..videoName
			        	--
			        	--print('==> Loading the video: '..videoName)
			        	-- TODO --
			        	-- now:     fixed frame rate
			        	-- future:  fixed frame #
			        	--
			        	local video = ffmpeg.Video{path=videoPath, width=opt.width, height=opt.height, fps=opt.fps, length=opt.seconds, delete=true, destFolder='out_frames',silent=true}
			        	--
			        	-- --video:play{} -- play the video
			        	local vidTensor = video:totensor{} -- read the whole video & turn it into a 4D tensor
				        --
				        local numFrame = vidTensor:size(1) -- Video frame number
				        --
				        if numFrame >= numFrameMin then
				          	----------------------------------------------
				          	--       Save the video name and path       --
				          	----------------------------------------------
				            local i,j = string.find(videoName,'_g') -- find the location of the group info in the string
				            local videoGroup = tonumber(string.sub(videoName,j+1,j+2)) -- get the group#
				            local videoPathLocal = nameClass[c]..'/'..videoName

				            if groupSplit[sp].setTe:eq(videoGroup):sum() == 0 then -- training data
				            	Tr.countVideo = Tr.countVideo + 1
				            	Tr.name[Tr.countVideo] = videoName
				            	Tr.path[Tr.countVideo] = videoPathLocal
				            	
				            	--table.insert(Tr, {name = videoName})
				            	--table.insert(Tr, {name = videoName, path = videoPath})
				            else -- testing data
				            	Te.countVideo = Te.countVideo + 1
				            	Te.name[Tr.countVideo] = videoName
				            	Te.path[Tr.countVideo] = videoPathLocal

				            	--table.insert(Te, {name = videoName})
				            	--table.insert(Te, {name = videoName, path = videoPath})
				            end
				        end
			      	end
			      	collectgarbage()
			    end
			    
				Tr.c_finished = c -- save the index
				Te.c_finished = c -- save the index
				print('Split: '..sp)
				-- print(Tr)
				-- print(Te)
			  	print('The elapsed time for the class '..nameClass[c]..': ' .. timerClass:time().real .. ' seconds')
			  	torch.save(outTrain[sp].name, Tr)
			  	torch.save(outTest[sp].name, Te)

			  	collectgarbage()
			  	print(' ')
			end
		end
	end

	print('The total elapsed time in the split '..sp..': ' .. timerAll:time().real .. ' seconds')
	print('The total training class numbers in the split'..sp..': ' .. Tr.countClass)
	print('The total training video numbers in the split'..sp..': ' .. Tr.countVideo)
	print('The total testing class numbers in the split'..sp..': ' .. Te.countClass)
	print('The total testing video numbers in the split'..sp..': ' .. Te.countVideo)
	print ' '

	Tr = nil
	Te = nil
	collectgarbage()
end
